# -*- coding: utf-8 -*-
"""severitycategorizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KQPjO-bdpTXQ8OQum1F_D1okmd_OdE4W
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('Labelonly_Mortality_Data.csv')

# Display the first few rows
print(data.head())

# Check data types
print(data.dtypes)

# Get dataset shape
print(f"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns.")

# Check for missing values
print(data.isnull().sum())

# Separate numerical and categorical columns
numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

print("Numerical Columns:", numerical_cols)
print("Categorical Columns:", categorical_cols)

# Check missing values in numerical columns
print(data[numerical_cols].isnull().sum())

# Fill missing numerical values with the median
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())

# Check missing values in numerical columns
print(data[numerical_cols].isnull().sum())

# Fill missing numerical values with the median
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())

sns.countplot(x='Severity_Label', data=data)
plt.title('Distribution of Severity Label')
plt.xlabel('1=Mild Severity,2=Moderate Severity,3=Serious,4=Critical Severity,0=Healthy')
plt.ylabel('Count')
plt.show()

# Plot the distribution of 'Age'
plt.figure(figsize=(8,6))
sns.histplot(data['age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Boxplot of Severity by Age
plt.figure(figsize=(8,6))
sns.boxplot(x='Severity_Label', y='age', data=data)
plt.title('Severity Label by Age')
plt.xlabel('1=Mild Severity,2=Moderate Severity,3=Serious,4=Critical Severity,0=Healthy')
plt.ylabel('Age')
plt.show()

# Calculate correlation matrix
corr_matrix = data[numerical_cols].corr()

# Plot heatmap
plt.figure(figsize=(15,12))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""# Computing the features correlation with severity outcome to pick out the most relevant to all severity categories"""

# Compute correlation of all numerical features with Severity Label
correlations = data.corr()['Severity_Label'].sort_values(ascending=False)
print(correlations)

"""# Since 0.1 seems to be a good threshold, we will select the features with higher absolute correlation values (|corr| > 0.1) for the model"""

import pandas as pd

# Example correlations Series
correlations = pd.Series({
    "Severity_Label": 1.000000,
    "Blood sodium": 0.343651,
    "Chloride": 0.293796,
    "age": 0.208543,
    "SP O2": 0.184224,
    "EF": 0.177668,
    "temperature": 0.169569,
    "PH": 0.153863,
    "deficiencyanemias": 0.148146,
    "MCHC": 0.084658,
    "MCH": 0.077858,
    "Systolic blood pressure": 0.070663,
    "gendera": 0.068802,
    "Bicarbonate": 0.066191,
    "Renal failure": 0.048363,
    "MCV": 0.045982,
    "Lymphocyte": 0.033401,
    "hypertensive": 0.032207,
    "atrialfibrillation": 0.006416,
    "group": 0.004601,
    "Platelets": 0.003168,
    "PCO2": 0.000326,
    "CHD with no MI": -0.001148,
    "Basophils": -0.006011,
    "Magnesium ion": -0.009817,
    "diabetes": -0.010890,
    "Hyperlipemia": -0.015081,
    "Neutrophils": -0.016840,
    "Urine output": -0.022486,
    "depression": -0.030233,
    "Creatine kinase": -0.042302,
    "COPD": -0.044445,
    "RDW": -0.055770,
    "Blood calcium": -0.056011,
    "ID": -0.061812,
    "Urea nitrogen": -0.066889,
    "INR": -0.070899,
    "NT-proBNP": -0.078297,
    "PT": -0.082283,
    "Respiratory rate": -0.097290,
    "Creatinine": -0.101010,
    "BMI": -0.104130,
    "glucose": -0.108759,
    "Leucocyte": -0.129215,
    "heart rate": -0.201205,
    "Anion gap": -0.233984,
    "Diastolic blood pressure": -0.299041,
    "Blood potassium": -0.317440,
    "hematocrit": -0.364119,
    "RBC": -0.364259,
    "Lactic acid": -0.373686
})

# Initialize an empty list to store relevant feature names
relevantFeatures = []

# Iterate over feature names and their correlation values
for feature, corr in correlations.items():
    if abs(corr) > 0.1 and abs(corr) != 1:
        relevantFeatures.append(feature)

# Print the list of relevant feature names
print("Relevant Features with |correlation| > 0.1 (excluding |1|):")
for feature in relevantFeatures:
    print(f"- {feature}: {correlations[feature]}")

selected_features = [
 'Blood sodium',
 'Chloride',
 'age',
 'SP O2',
 'EF',
 'temperature',
 'PH',
 'deficiencyanemias',
 'Creatinine',
 'BMI',
 'glucose',
 'Leucocyte',
 'heart rate',
 'Anion gap',
 'Diastolic blood pressure',
 'Blood potassium',
 'hematocrit',
 'RBC',
 'Lactic acid',
]

# Ensure all selected features are present in your data
for feature in selected_features:
    if feature not in data.columns:
        print(f"Feature '{feature}' not found in the dataset.")

# Feature matrix
X = data[selected_features]

# Target variable
y = data['Severity_Label']

# Check for missing values
print(X.isnull().sum())

# Fill missing numerical values with median
X = X.fillna(X.median())

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the feature matrix
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

# Split data: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

from sklearn.linear_model import LogisticRegression

# Initialize the model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Train the model
logreg.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Display confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Get feature names after encoding
feature_names = X.columns

# Create a DataFrame for coefficients
coefficients = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': logreg.coef_[0]
})

# Sort by absolute value of coefficient
coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()
coefficients = coefficients.sort_values(by='Abs_Coefficient', ascending=False)
print(coefficients[['Feature', 'Coefficient']])

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf:.2f}")

print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

from sklearn.model_selection import cross_val_score

# Logistic Regression cross-validation
logreg_cv_scores = cross_val_score(logreg, X_scaled, y, cv=5)
print(f"Logistic Regression CV Scores: {logreg_cv_scores}")
print(f"Average CV Score: {logreg_cv_scores.mean():.2f}")

# Random Forest cross-validation
rf_cv_scores = cross_val_score(rf, X_scaled, y, cv=5)
print(f"Random Forest CV Scores: {rf_cv_scores}")
print(f"Average CV Score: {rf_cv_scores.mean():.2f}")

from sklearn.model_selection import GridSearchCV

# Define parameter grid for Logistic Regression
param_grid_logreg = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}

# Grid search for Logistic Regression
grid_logreg = GridSearchCV(
    LogisticRegression(max_iter=1000, random_state=42),
    param_grid_logreg, cv=5, scoring='accuracy'
)
grid_logreg.fit(X_train, y_train)
print(f"Best parameters for Logistic Regression: {grid_logreg.best_params_}")

# Define parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'max_features': ['auto', 'sqrt']
}

# Grid search for Random Forest
grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf, cv=5, scoring='accuracy'
)
grid_rf.fit(X_train, y_train)
print(f"Best parameters for Random Forest: {grid_rf.best_params_}")

# Retrain Logistic Regression with best parameters
best_logreg = grid_logreg.best_estimator_
best_logreg.fit(X_train, y_train)
y_pred_best_logreg = best_logreg.predict(X_test)
accuracy_best_logreg = accuracy_score(y_test, y_pred_best_logreg)
print(f"Optimized Logistic Regression Accuracy: {accuracy_best_logreg:.2f}")

# Retrain Random Forest with best parameters
best_rf = grid_rf.best_estimator_
best_rf.fit(X_train, y_train)
y_pred_best_rf = best_rf.predict(X_test)
accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
print(f"Optimized Random Forest Accuracy: {accuracy_best_rf:.2f}")

import joblib

# Save the best model
joblib.dump(logreg, 'best_logistic_regression_model.pkl')

# Load the model
loaded_model = joblib.load('best_logistic_regression_model.pkl')

# Save the fitted StandardScaler
joblib.dump(scaler, 'scaler.pkl')
print("StandardScaler saved as 'scaler.pkl'.")

# Make predictions with the loaded model
sample_data = X_test[:1]  # Replace with your sample input
print(sample_data)
prediction = loaded_model.predict(sample_data)
print(f"Predicted Outcome: {prediction[0]}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming 'y_test' is your actual outcomes
# 'y_pred_best_logreg' is the predicted outcomes from the logistic regression model

# Compute the confusion matrix
cm_logreg = confusion_matrix(y_test, y_pred_best_logreg)

# Get unique labels from y_test (or y_train) to ensure all classes are included
unique_labels = sorted(y_test.unique())

# Define display labels corresponding to your unique labels
# You may need to adjust these labels according to your problem
display_labels = [str(label) for label in unique_labels]  # Convert to strings if necessary

# Plot the confusion matrix with updated display_labels
disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg, display_labels=display_labels)
disp_logreg.plot(cmap='Blues')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()